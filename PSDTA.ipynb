{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PSDTA Data Preparation\n",
    "\n",
    "Your CSV should look something like this:\n",
    "\n",
    "| ID |  Protein  |          Ligand          | Target_Chain | regression_label |\n",
    "|:----------:|:---------:|:------------------------:|:------------:|:----------------:|\n",
    "| 1a30| PQITL.... |       CC(C)C[C...        |      A       |       4.3        |\n",
    "| 1bcu| ADCGL.... | Nc1ccc2cc3ccc(N)cc3nc2c1 |      B       |       3.8        |\n",
    "|...|    ...    |           ...            |     ...      |       ...        |\n",
    "|1bzc| TEMEKE... |        NC(=O)...         |      A       |       6.6        |\n",
    "\n",
    "You also need to prepare the PDB file corresponding to each protein.\n"
   ],
   "id": "3189bbfd0d267c47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PSDTA Hyperparameter Tuning\n",
    "\n",
    "The config.json file contains the hyperparameter settings.\n",
    "\n",
    "When the dataset is small, the attention mechanism can be set to 2 layers.\n",
    "\n",
    "For larger datasets, it is recommended to set it to 3 layers.\n",
    "\n",
    "For PNA, different combinations of aggregators show varying performance. Currently, the combination of mean + min achieves the best results."
   ],
   "id": "c1a8b4180599f838"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    \"params\": {\n",
    "        \"mol_in_channels\": 43,\n",
    "        \"prot_in_channels\": 33,\n",
    "        \"prot_evo_channels\": 1280,\n",
    "        \"hidden_channels\":200,\n",
    "        \"aggregators\": [\n",
    "            \"mean\",\n",
    "            \"min\"\n",
    "        ],\n",
    "        \"scalers\": [\n",
    "            \"identity\",\n",
    "            \"amplification\",\n",
    "            \"linear\"\n",
    "        ],\n",
    "        \"pre_layers\": 2,\n",
    "        \"post_layers\": 1,\n",
    "        \"total_layer\": 3,\n",
    "        \"K\": [\n",
    "            5,\n",
    "            10,\n",
    "            20\n",
    "        ],\n",
    "        \"dropout\": 0,\n",
    "        \"dropout_attn_score\": 0,\n",
    "        \"heads\": 5\n",
    "    }\n",
    "}\n"
   ],
   "id": "66ef404c34b9aefa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5e5a0effde1aad99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In addition, some hyperparameters are also defined in main.py.",
   "id": "2395c86cfd2be7d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "acc281c3a7d59943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Seed and device\n",
    "parser.add_argument('--seed', type=int, default=2)\n",
    "parser.add_argument('--device', type=str, default='cuda:0', help='')\n",
    "parser.add_argument('--config_path', type=str, default='config.json')\n",
    "### Data and Pre-processing\n",
    "parser.add_argument('--datafolder', type=str, default='./dataset/davis/', help='protein data path')  \n",
    "parser.add_argument('--result_path', type=str, default='./result/', help='path to save results') \n",
    "parser.add_argument('--save_interpret', type=bool, default=True, help='path to save results')\n",
    "\n",
    "# For PDBBIND datasets - we train for 30K iteration\n",
    "parser.add_argument('--regression_task', type=bool, help='True if regression else False')\n",
    "# For any classification type - we train for 100 epochs (same as DrugBAN) [change --total_iters = None]\n",
    "parser.add_argument('--classification_task', type=bool, help='True if classification else False')\n",
    "parser.add_argument('--mclassification_task', type=int, help='number of multiclassification, 0 if no multiclass task')\n",
    "parser.add_argument('--epochs', type=int, default=200 , help='')\n",
    "parser.add_argument('--evaluate_epoch', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--total_iters', type=int, default=None)\n",
    "parser.add_argument('--evaluate_step', type=int, default=500)\n",
    "\n",
    "# optimizer params - only change this for PDBBind v2016\n",
    "parser.add_argument('--lrate', type=float, default=1e-5,  #change to 1e-5 for PDBv2016  1e-4 for PDB2020\n",
    "                    help='learning rate for PSICHIC')  # change to 1e-5 for LargeScaleInteractionDataset\n",
    "parser.add_argument('--eps', type=float, default=1e-5, help='higher = closer to SGD')  # change to 1e-5 for PDBv2016, 1e-08 for PDB2020\n",
    "parser.add_argument('--betas', type=tuple_type, default=\"(0.9,0.999)\")  # change to (0.9,0.99) for PDBv2016  (0.9,0.999) for PDB2020\n",
    "# batch size\n",
    "parser.add_argument('--batch_size', type=int, default=14)\n",
    "# sampling method - only used for pretraining large-scale interaction dataset ; allow self specified weights to the samples\n",
    "parser.add_argument('--sampling_col', type=str, default='')\n",
    "parser.add_argument('--trained_model_path', type=str, default='', #./result/PDB2016_BENCHMARK/save_model_seed2/\n",
    "                    help='This does not need to be perfectly aligned, as you can add prediction head for some other tasks as well!')\n",
    "parser.add_argument('--finetune_modules', type=list_type, default=None)\n",
    "# notebook mode?\n",
    "parser.add_argument('--nb_mode', type=bool, default=False)"
   ],
   "id": "af9904cf1ccdc4a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running PSDTA\n",
    "\n",
    "Once the feature files are prepared, specify the feature file location and the result path, then run main.py to start training the model.\n",
    "\n",
    "For example, for PDBBind2016:"
   ],
   "id": "f3b91976caf5ec66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "python main.py --datafolder dataset/pdb2016 --result_path result/PDB2016_BENCHMARK --regression_task True\n",
   "id": "b7dc3d61afe9832a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "# Utils\n",
    "from utils.utils import DataLoader, compute_pna_degrees, virtual_screening, CustomWeightedRandomSampler\n",
    "from utils.dataset import *  # data\n",
    "from utils.trainer import Trainer\n",
    "from utils.metrics import *\n",
    "from utils.utils import extract_data_from_files\n",
    "# Preprocessing\n",
    "from utils import protein_init, ligand_init\n",
    "# Model\n",
    "# from models.net import net\n",
    "import argparse\n",
    "import ast\n",
    "from AAA.net_tsmiles import net_tsmiles\n",
    "# from BBB.sub_net import net\n",
    "# from BBB.sub_trainer import sub_Trainer\n",
    "from BAN.ban_net import net\n",
    "from utils.draw import Trainer_draw\n",
    "from utils.eval_pocket import Trainer_eval_pocket\n",
    "from Protein_family.family_trainer import Trainer_family\n",
    "\n",
    "def tuple_type(s):\n",
    "    try:\n",
    "        # Safely evaluate the string as a tuple\n",
    "        value = ast.literal_eval(s)\n",
    "        if not isinstance(value, tuple):\n",
    "            raise ValueError\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise argparse.ArgumentTypeError(f\"Invalid tuple value: {s}\")\n",
    "    return value\n",
    "\n",
    "\n",
    "def list_type(s):\n",
    "    try:\n",
    "        # Safely evaluate the string as a tuple\n",
    "        value = ast.literal_eval(s)\n",
    "        if not isinstance(value, list):\n",
    "            raise ValueError\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise argparse.ArgumentTypeError(f\"Invalid list value: {s}\")\n",
    "    return value\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "### Seed and device\n",
    "parser.add_argument('--seed', type=int, default=2) #最好的是2\n",
    "parser.add_argument('--device', type=str, default='cuda:0', help='')\n",
    "parser.add_argument('--config_path', type=str, default='config.json')\n",
    "### Data and Pre-processing\n",
    "parser.add_argument('--datafolder', type=str, default='./dataset/davis/', help='protein data path')  #运行时有指定，此处无所谓\n",
    "parser.add_argument('--result_path', type=str, default='./result/', help='path to save results')  #运行时有指定，此处无所谓\n",
    "parser.add_argument('--save_interpret', type=bool, default=True, help='path to save results')\n",
    "\n",
    "# For PDBBIND datasets - we train for 30K iteration\n",
    "parser.add_argument('--regression_task', type=bool, help='True if regression else False')\n",
    "# For any classification type - we train for 100 epochs (same as DrugBAN) [change --total_iters = None]\n",
    "parser.add_argument('--classification_task', type=bool, help='True if classification else False')\n",
    "parser.add_argument('--mclassification_task', type=int, help='number of multiclassification, 0 if no multiclass task')\n",
    "parser.add_argument('--epochs', type=int, default=200 , help='')\n",
    "parser.add_argument('--evaluate_epoch', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--total_iters', type=int, default=None)\n",
    "parser.add_argument('--evaluate_step', type=int, default=500)\n",
    "\n",
    "# optimizer params - only change this for PDBBind v2016\n",
    "parser.add_argument('--lrate', type=float, default=1e-5,  #change to 1e-5 for PDBv2016  1e-4 for PDB2020\n",
    "                    help='learning rate for PSICHIC')  # change to 1e-5 for LargeScaleInteractionDataset\n",
    "parser.add_argument('--eps', type=float, default=1e-5, help='higher = closer to SGD')  # change to 1e-5 for PDBv2016, 1e-08 for PDB2020\n",
    "parser.add_argument('--betas', type=tuple_type, default=\"(0.9,0.999)\")  # change to (0.9,0.99) for PDBv2016  (0.9,0.999) for PDB2020\n",
    "# batch size\n",
    "parser.add_argument('--batch_size', type=int, default=14)\n",
    "# sampling method - only used for pretraining large-scale interaction dataset ; allow self specified weights to the samples\n",
    "parser.add_argument('--sampling_col', type=str, default='')\n",
    "parser.add_argument('--trained_model_path', type=str, default='', #./result/PDB2016_BENCHMARK/save_model_seed2/\n",
    "                    help='This does not need to be perfectly aligned, as you can add prediction head for some other tasks as well!')\n",
    "parser.add_argument('--finetune_modules', type=list_type, default=None)\n",
    "# notebook mode?\n",
    "parser.add_argument('--nb_mode', type=bool, default=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 根据文件地址构建config\n",
    "if args.trained_model_path:\n",
    "    with open(args.config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "else:\n",
    "    with open(os.path.join(args.trained_model_path, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "# overwrite\n",
    "config['optimizer']['lrate'] = args.lrate  #parser\n",
    "config['optimizer']['eps'] = args.eps #1e-8  #parser\n",
    "config['optimizer']['betas'] = args.betas  ##parser (0.9,0.999)\n",
    "config['tasks']['regression_task'] = args.regression_task  #parser 回归任务\n",
    "config['tasks']['classification_task'] = args.classification_task  #parser 分类任务\n",
    "config['tasks']['mclassification_task'] = args.mclassification_task  #parser 多分类任务\n",
    "\n",
    "# device\n",
    "device = torch.device(args.device)\n",
    "if not os.path.exists(args.result_path):\n",
    "    os.makedirs(args.result_path)  # 保存结果\n",
    "\n",
    "model_path = os.path.join(args.result_path, 'save_model_seed{}'.format(args.seed))\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)  # 保存模型\n",
    "\n",
    "interpret_path = os.path.join(args.result_path, 'interpretation_result_seed{}'.format(args.seed))\n",
    "if not os.path.exists(interpret_path):\n",
    "    os.makedirs(interpret_path)  # 保存解释结果\n",
    "\n",
    "if args.epochs is not None and args.total_iters is not None:\n",
    "    print('If epochs and total iters are both not None, then we only use iters.')\n",
    "    args.epochs = None\n",
    "\n",
    "print(args)\n",
    "with open(os.path.join(args.result_path, 'model_params.txt'), 'w') as f:\n",
    "    f.write(str(args))\n",
    "\n",
    "# seed initialize\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "random.seed(args.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "\n",
    "\n",
    "## 2016 import files\n",
    "train_file=os.path.join(args.datafolder, 'train.csv')\n",
    "valid_file=os.path.join(args.datafolder, 'test.csv') #valid.csv\n",
    "test_file=os.path.join(args.datafolder, 'test.csv')  #test.csv\n",
    "train_df = pd.read_csv(train_file)  # datafolder是路径\n",
    "test_df = pd.read_csv(test_file)\n",
    "valid_path = os.path.join(args.datafolder, 'test.csv')\n",
    "valid_df = None\n",
    "files = [train_file, valid_file, test_file]  # CSV文件路径列表\n",
    "\n",
    "\n",
    "if os.path.exists(valid_path):  # 如果有验证集\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "    protein_tuples = extract_data_from_files(files)\n",
    "    ligand_smiles = list(\n",
    "        set(train_df['Ligand'].tolist() + test_df['Ligand'].tolist() + valid_df['Ligand'].tolist()))  # 配体SMILES串\n",
    "else:\n",
    "    protein_tuples = extract_data_from_files(files)\n",
    "    ligand_smiles = list(set(train_df['Ligand'].tolist() + test_df['Ligand'].tolist()))\n",
    "\n",
    "protein_path = os.path.join(args.datafolder, 'protein.pt')  #davis 是protein_v2.pt,2020是protein.pt\n",
    "\n",
    "if os.path.exists(protein_path):\n",
    "    print('Loading Protein Graph data...')\n",
    "    protein_dict = torch.load(protein_path)\n",
    "else:\n",
    "    print('Initialising Protein Sequence to Protein Graph...')\n",
    "    protein_dict = protein_init(protein_tuples)  # 得到序列到蛋白质图（包括序列、序列特征、token特征、残基数、位置索引、邻接索引、邻接权重）\n",
    "    torch.save(protein_dict, protein_path)  # 保存数据\n",
    "\n",
    "ligand_path = os.path.join(args.datafolder, 'ligand.pt')\n",
    "if os.path.exists(ligand_path):\n",
    "    print('Loading Ligand Graph data...')\n",
    "    ligand_dict = torch.load(ligand_path)\n",
    "else:\n",
    "    print('Initialising Ligand SMILES to Ligand Graph...')\n",
    "    ligand_dict = ligand_init(ligand_smiles)\n",
    "    torch.save(ligand_dict, ligand_path)\n",
    "\n",
    "torch.cuda.empty_cache()  #清空 PyTorch 在 GPU 上分配但不再使用的缓存内存\n",
    "##TODO: drop any invalid smiles\n",
    "\n",
    "##TODO: drop any invalid smiles\n",
    "\n",
    "##\n",
    "## training loader\n",
    "train_shuffle = True\n",
    "train_sampler = None\n",
    "\n",
    "if args.sampling_col:\n",
    "    train_weights = torch.from_numpy(train_df[args.sampling_col].values)  # 采样权重\n",
    "\n",
    "\n",
    "    def sampler_from_weights(weights):\n",
    "        sampler = CustomWeightedRandomSampler(weights, len(weights), replacement=True)  # 根据权重定义采样器，并允许多次采样同一个值\n",
    "\n",
    "        return sampler\n",
    "\n",
    "\n",
    "    train_shuffle = False\n",
    "    train_sampler = sampler_from_weights(train_weights)\n",
    "\n",
    "if train_sampler is not None:\n",
    "    print('shuffle should be False: ', train_shuffle)\n",
    "\n",
    "train_dataset = ProteinMoleculeDataset(train_df, ligand_dict, protein_dict,\n",
    "                                       device=args.device)  # train_dataset就是一个包含多个MultiGraphData对象的集合\n",
    "test_dataset = ProteinMoleculeDataset(test_df, ligand_dict, protein_dict, device=args.device)\n",
    "\n",
    "# DataLoader需要为这三个特征(分子特征、原子所属团簇、蛋白质序列特征)创建单独的索引区分其属于不同的图。DataLoder会将一个批次中的独立数据进行拼接，每个数据可能大小不同，所以需要batch来区分数据原本属于哪个图\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=train_shuffle,\n",
    "                          sampler=train_sampler, follow_batch=['mol_x', 'clique_x', 'prot_node_aa','tsml'])\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                         follow_batch=['mol_x', 'clique_x', 'prot_node_aa'])\n",
    "\n",
    "valid_dataset, valid_loader = None, None\n",
    "if valid_df is not None:\n",
    "    valid_dataset = ProteinMoleculeDataset(valid_df, ligand_dict, protein_dict, device=args.device)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                              follow_batch=['mol_x', 'clique_x', 'prot_node_aa','tsml']\n",
    "                              )\n",
    "\n",
    "\n",
    "if not args.trained_model_path:\n",
    "    degree_path = os.path.join(args.datafolder, 'degree.pt')\n",
    "    if not os.path.exists(degree_path):\n",
    "        print('Computing training data degrees for PNA')\n",
    "        mol_deg, clique_deg, prot_deg = compute_pna_degrees(train_loader)  # 度的直方图张量，反应不同度的频率\n",
    "        degree_dict = {'ligand_deg': mol_deg, 'clique_deg': clique_deg, 'protein_deg': prot_deg}  # 封装成字典\n",
    "    else:\n",
    "        degree_dict = torch.load(degree_path)\n",
    "        mol_deg, clique_deg, prot_deg = degree_dict['ligand_deg'], degree_dict['clique_deg'], degree_dict['protein_deg']\n",
    "\n",
    "    torch.save(degree_dict, os.path.join(args.result_path, 'save_model_seed{}'.format(args.seed), 'degree.pt'))\n",
    "else:\n",
    "    degree_dict = torch.load(os.path.join(args.trained_model_path, 'degree.pt'))\n",
    "    param_dict = os.path.join(args.trained_model_path, 'best.pt') #model_test\n",
    "    mol_deg, prot_deg = degree_dict['ligand_deg'], degree_dict['protein_deg']\n",
    "\n",
    "model = net(mol_deg, prot_deg,\n",
    "            # MOLECULE\n",
    "            mol_in_channels=config['params']['mol_in_channels'], prot_in_channels=config['params']['prot_in_channels'],\n",
    "            prot_evo_channels=config['params']['prot_evo_channels'],\n",
    "            hidden_channels=config['params']['hidden_channels'], pre_layers=config['params']['pre_layers'],\n",
    "            post_layers=config['params']['post_layers'], aggregators=config['params']['aggregators'],\n",
    "            scalers=config['params']['scalers'], total_layer=config['params']['total_layer'],\n",
    "            K=config['params']['K'], heads=config['params']['heads'],\n",
    "            dropout=config['params']['dropout'],\n",
    "            dropout_attn_score=config['params']['dropout_attn_score'],\n",
    "            # output\n",
    "            regression_head=config['tasks']['regression_task'],\n",
    "            classification_head=config['tasks']['classification_task'],\n",
    "            multiclassification_head=config['tasks']['mclassification_task'],\n",
    "            device=device).to(device)\n",
    "\n",
    "\n",
    "model.reset_parameters()\n",
    "if args.trained_model_path:\n",
    "    model.load_state_dict(torch.load(param_dict, map_location=args.device), strict=False)\n",
    "    print('Pretrained model loaded!!!')\n",
    "\n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('Model loaded with number of parameters being:', str(nParams))\n",
    "\n",
    "with open(os.path.join(args.result_path, 'save_model_seed{}'.format(args.seed), 'config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "\n",
    "evaluation_metric = 'rmse'\n",
    "\n",
    "engine = Trainer(model=model, lrate=config['optimizer']['lrate'], min_lrate=config['optimizer']['min_lrate'],\n",
    "                 wdecay=config['optimizer']['weight_decay'], betas=config['optimizer']['betas'],\n",
    "                 eps=config['optimizer']['eps'], amsgrad=config['optimizer']['amsgrad'],\n",
    "                 clip=config['optimizer']['clip'], steps_per_epoch=len(train_loader),\n",
    "                 num_epochs=args.epochs, total_iters=args.total_iters,\n",
    "                 warmup_iters=config['optimizer']['warmup_iters'],\n",
    "                 lr_decay_iters=config['optimizer']['lr_decay_iters'],\n",
    "                 schedule_lr=config['optimizer']['schedule_lr'], regression_weight=1, classification_weight=1,\n",
    "                 evaluate_metric=evaluation_metric, result_path=args.result_path, runid=args.seed,\n",
    "                 finetune_modules=args.finetune_modules,\n",
    "                 device=device)\n",
    "\n",
    "print('-' * 50)\n",
    "print('start training model')\n",
    "if args.epochs:\n",
    "    engine.train_epoch(train_loader, val_loader=valid_loader, test_loader=test_loader,\n",
    "                       evaluate_epoch=args.evaluate_epoch)\n",
    "else:\n",
    "    engine.train_step(train_loader, val_loader=valid_loader, test_loader=test_loader, evaluate_step=args.evaluate_step)\n",
    "\n",
    "print('finished training model')\n",
    "print('-' * 50)\n",
    "\n",
    "print('loading best checkpoint and predicting test data')\n",
    "print('-' * 50)\n",
    "stat_dict_path=os.path.join(args.result_path, 'save_model_seed{}'.format(args.seed), 'model.pt')\n",
    "model.load_state_dict(torch.load())\n",
    "screen_df = virtual_screening(test_df, model, test_loader,\n",
    "                              result_path=os.path.join(args.result_path,\n",
    "                                                       \"interpretation_result_seed{}\".format(args.seed)),\n",
    "                              save_interpret=args.save_interpret,\n",
    "                              ligand_dict=ligand_dict, device=args.device)\n",
    "\n",
    "screen_df.to_csv(os.path.join(args.result_path, 'test_prediction_seed{}.csv'.format(args.seed)), index=False)\n",
    "\n",
    "#-----画图------------------\n",
    "# model.reset_parameters()\n",
    "#\n",
    "# param_dict = os.path.join(args.trained_model_path, 'model_test.pt')\n",
    "# model.load_state_dict(torch.load(param_dict, map_location=args.device), strict=False)\n",
    "# draw_train =  Trainer_draw(model=model, device=device)\n",
    "# # draw_train.draw(train_loader, valid_loader, test_loader, args.result_path)\n",
    "# eval_result = draw_train.test_epoch(test_loader)\n",
    "# print(eval_result)\n",
    "\n",
    "#-----打印口袋准确率--------------\n",
    "# model.reset_parameters()\n",
    "# param_dict = os.path.join(args.trained_model_path, 'model_test.pt')\n",
    "# model.load_state_dict(torch.load(param_dict, map_location=args.device), strict=False)\n",
    "# eval_pocket_train =  Trainer_eval_pocket(model=model, device=device)\n",
    "# eval_result = eval_pocket_train.test_epoch(test_loader)\n",
    "# print(eval_result)\n",
    "\n",
    "#------蛋白质家族测试-------------\n",
    "# model.reset_parameters()\n",
    "# param_dict = os.path.join(args.trained_model_path, 'model_test.pt')\n",
    "# model.load_state_dict(torch.load(param_dict, map_location=args.device), strict=False)\n",
    "# draw_train =  Trainer_family(model=model, device=device)\n",
    "# eval_result = draw_train.test_epoch(test_loader)"
   ],
   "id": "a189a690af18cfe6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
